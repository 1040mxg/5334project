{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5334proj",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP5quG+L1DP4K2ORAw9n7s4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1040mxg/5334project/blob/main/5334proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMYGyMoit6up"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKGTfUbLt92h"
      },
      "source": [
        "[Blog homepage](https://1040mxg.github.io/blog/) || [View on Github](https://github.com/1040mxg/blog/blob/gh-pages/_posts/2021-04-27-nbc.md)\n",
        "\n",
        "The goal of this assignment is to learn about Naive Bayes Classifiers by building a classifier and training and testing it on an IMDB dataset with the goal of predicting whether a given review's sentiment is \"Positive\" or \"Negative\".\n",
        "\n",
        "**Naive Bayes** is a simple, fast, and accurate algorithm that works particularly well with natural language processing (NLP), or text classification. In my tests, I used the **Multinomial Naive Bayes** algorithm. It works by taking advantage of probability theory and Bayes' Theorem to classify text. For each piece of input text, the probability of each possible class is calculated and the final classification made based on highest probability.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAnhYFeqNXi4",
        "outputId": "a4728b14-0b75-4ece-f529-c26e08514d09"
      },
      "source": [
        "pip install --user -U nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: nltk in /root/.local/lib/python3.7/site-packages (3.6.2)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZcx64r1eoMK"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5n-lDNHewYU"
      },
      "source": [
        "Import IMDB dataset and prepare a data array for later graphing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVyQhmWixBWH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a1d0e7c-eaf9-4d52-bbf8-b389e9a809a9"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk import ngrams\n",
        "from collections import Counter\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "\n",
        "##import data file and print some basic stats\n",
        "file = \"tweets_xh_hk.txt\"\n",
        "file2 = \"tweets_npr_hk.txt\"\n",
        "xhData = []\n",
        "nprData = []\n",
        "\n",
        "with open(file, 'r') as f:\n",
        "  for row in f:\n",
        "    xhData.append(row)\n",
        "\n",
        "with open(file2, 'r') as f:\n",
        "  for row in f:\n",
        "    nprData.append(row)\n",
        "\n",
        "data = []\n",
        "\n",
        "for i in range(len(xhData)):\n",
        "  if len(xhData[i]) > 25:\n",
        "    data.append([xhData[i], '0'])\n",
        "\n",
        "for i in range(len(nprData)):\n",
        "  if len(nprData[i]) > 25:\n",
        "    data.append([nprData[i], '1'])\n",
        "\n",
        "X = []\n",
        "y = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr2KEyyNumBa"
      },
      "source": [
        "Divide the dataset into train, dev, and test sets. First, the data is shuffled and then divided manually using index numbers. To start, the data was divided in an 80/20 ratio for the initial train and test set. Then, the training set was further divided again in an 80/20, train/dev ratio.\n",
        "\n",
        "The final sizes for each dataset are as follows:\n",
        "*   train = 640 items\n",
        "*   dev = 160 items\n",
        "*   tets = 200 items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzyTQC5l_LZ2",
        "outputId": "1e9c15ac-5f88-46cc-9fe7-1cdbd576da30"
      },
      "source": [
        "import random\n",
        "\n",
        "print(\"XH:\", len(xhData))\n",
        "print(\"NPR:\", len(nprData))\n",
        "\n",
        "random.seed(1)\n",
        "random.shuffle(data)\n",
        "dataLen = len(data)\n",
        "\n",
        "train = []\n",
        "dev = []\n",
        "test =[]\n",
        "\n",
        "#test = 680items\n",
        "\n",
        "print(dataLen)\n",
        "testNum = 680\n",
        "trainNum = 2182\n",
        "devNum = 545\n",
        "\n",
        "for i in range(trainNum):\n",
        "  train.append(data[i])\n",
        "\n",
        "for i in range(trainNum, trainNum+devNum):\n",
        "  dev.append(data[i])\n",
        "\n",
        "for i in range(dataLen-testNum, dataLen):\n",
        "  test.append(data[i])\n",
        "\n",
        "print(\"Train Data: %d items\" %(len(train)))\n",
        "print(\"Dev Data: %d items\" %(len(dev)))\n",
        "print(\"Test Data: %d items\" %(len(test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XH: 2893\n",
            "NPR: 514\n",
            "3232\n",
            "Train Data: 2182 items\n",
            "Dev Data: 545 items\n",
            "Test Data: 680 items\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0Lh-J39H3lS"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ1Tj1tBe4az"
      },
      "source": [
        "Using regular expressions, the training set of 640 items (lines) is split into two lists based on positive/negative sentiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9Ljw_8QEvy1",
        "outputId": "4587adf0-f072-48d9-d823-0e8fd1b1f660"
      },
      "source": [
        "import re\n",
        "\n",
        "#pos = re.search(\"1$\")\n",
        "#neg = re.search(\"0$\")\n",
        "\n",
        "#split trainset into pos + negative using regular expressions\n",
        "def splitPosNeg(data):\n",
        "  posStr = []\n",
        "  negStr = []\n",
        "  for i in range(len(data)):\n",
        "    if data[i][-1] == '1':\n",
        "      posStr.append(data[i][0])\n",
        "    elif data[i][-1] == '0':\n",
        "      negStr.append(data[i][0])\n",
        "  return posStr, negStr\n",
        "\n",
        "def printPosNeg(posStr, negStr):\n",
        "  print(\"Positive Reviews: %d items\" %(len(posStr)))\n",
        "  print(\"---------------Sample---------------\")\n",
        "  for i in range(5):\n",
        "    if(len(posStr[i])>80):\n",
        "      print(\"%s... 1\" %posStr[i][0:80])\n",
        "    else:\n",
        "      print(posStr[i], end='')\n",
        "\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"Negative Reviews: %d items\" %(len(negStr)))\n",
        "  print(\"---------------Sample---------------\")\n",
        "  for i in range(5):\n",
        "    if(len(negStr[i])>80):\n",
        "      print(\"%s... 0\" %negStr[i][0:80])\n",
        "    else:\n",
        "      print(negStr[i], end='')\n",
        "\n",
        "posStr, negStr = splitPosNeg(train)\n",
        "printPosNeg(posStr, negStr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Positive Reviews: 235 items\n",
            "---------------Sample---------------\n",
            "Beijing has enacted a controversial new security law in Hong Kong. It criminaliz... 1\n",
            "Hong Kong Police Use Tear Gas On Large Pro-Democracy Protest n.pr/1uTEaeZ\n",
            "For months, Hong Kong has been wracked by protests. What are the demonstrators f... 1\n",
            "President Trump said he supports pro-democracy demonstrators in Hong Kong — but ... 1\n",
            "NPRs @EmilyZFeng live in Hong Kong: twitter.com/EmilyZFeng/sta…\n",
            "\n",
            "\n",
            "Negative Reviews: 1947 items\n",
            "---------------Sample---------------\n",
            "Welcome to @Disneyland in Hong Kong. You gotta plenty to enjoy in this sparkling... 0\n",
            "U.S. Congress' passage of the so-called Hong Kong Human Rights and Democracy Act... 0\n",
            "\"#UPDATE: 52 countries welcome China's adoption of law on safeguarding national ... 0\n",
            "Check out our #AsiaAlbum series here xhne.ws/uKix6 https://t.co/M22mzxvNkC\"\n",
            "The Committee for Safeguarding National Security of the Hong Kong Special Admini... 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs-ckUGRJ4Ls"
      },
      "source": [
        "The positive/negative review lists are further processed into dictionaries of vocabularies.\n",
        "\n",
        "First, each line in the lists was split into individual words and added to a temporary dictionary. This temporary dictionary was then trimmed to include only words that occur more than 3 times, and are greater than 3 characters in length to help filter out articles like \"a\", \"an\", as well as the classifying 1/0. The reasoning behind this was to focus on words that contribute to a sentiment, without filler words in the data. This unfortunately leaves in filler words like \"the\" or \"and\", but leaves in potentially important words like \"bad\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnnuiVkUK3DV",
        "outputId": "f135d36b-dee8-48f6-f2d8-14a81b13c550"
      },
      "source": [
        "#trainset vocab list + split into pos/neg\n",
        "def splitWord(strings, tempDict):\n",
        "  words = 0\n",
        "  for line in strings:\n",
        "    line = line.lower()\n",
        "    temp = re.findall(r'\\w+', line)\n",
        "    for word in temp:\n",
        "      words+=1\n",
        "      if word in tempDict:\n",
        "        tempDict[word] +=1\n",
        "      else:\n",
        "        tempDict[word] = 1\n",
        "  #print(\"Total Word Count: \", words)\n",
        "\n",
        "\"\"\"\n",
        "remove all occurrences < 3, all words 2 characters or less to take care of articles like \"a\", \"an\", as well as the classifying 1/0\n",
        "\"\"\"\n",
        "def trim(strings, tempDict):\n",
        "  keys = list(tempDict.keys())\n",
        "  vals = list(tempDict.values())\n",
        "  newDict = dict()\n",
        "  count = 0\n",
        "  for i in range(len(vals)):\n",
        "    k = keys[i]\n",
        "    if vals[i] >= 3 and k not in stopwords:\n",
        "      newDict[k] = vals[i]\n",
        "      count+=vals[i]\n",
        "  return newDict, count\n",
        "\n",
        "def makeVocab(strings):\n",
        "  tempDict = dict()\n",
        "  splitWord(strings, tempDict)\n",
        "  newDict, count = trim(strings, tempDict)\n",
        "  return newDict, count\n",
        "\n",
        "print(\"---------------Positive Vocabulary---------------\")\n",
        "posVocab, posCount = makeVocab(posStr)\n",
        "print(\"Words After Trimming: \", posCount)\n",
        "print(\"Unique Words: %d\\n\" %len(posVocab))\n",
        "\n",
        "print(\"---------------Negative Vocabulary---------------\")\n",
        "negVocab, negCount = makeVocab(negStr)\n",
        "print(\"Words After Trimming: \", negCount)\n",
        "print(\"Unique Words: %d\\n\" %len(negVocab))\n",
        "\n",
        "print(\"---------------All Words---------------\")\n",
        "allVocab = {**posVocab, **negVocab}\n",
        "allCount = posCount+negCount\n",
        "print(\"Total Word Count: \", allCount)\n",
        "print(\"Unique Words: \", len(allVocab))\n",
        "\n",
        "# print(\"Positive:\", posVocab)\n",
        "# print(\"Negative:\", negVocab)\n",
        "# print(\"All:\", allVocab)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------Positive Vocabulary---------------\n",
            "Words After Trimming:  1883\n",
            "Unique Words: 210\n",
            "\n",
            "---------------Negative Vocabulary---------------\n",
            "Words After Trimming:  29445\n",
            "Unique Words: 1802\n",
            "\n",
            "---------------All Words---------------\n",
            "Total Word Count:  31328\n",
            "Unique Words:  1856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IXl4rFhBcS6"
      },
      "source": [
        "P(positive) and P(negative) were calculated by dividing the count of each by the total (trimmed) word count.\n",
        "\n",
        "Individual P(word|sentiment) were calculated using (#word in class)/(#total word count in sentiment class). A general P(word) was also calculated using (#word/#total word count). All three were stored in new dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5Jx9GQf_pfE",
        "outputId": "884a692d-ddee-4ffa-9570-cfe3f5a651c4"
      },
      "source": [
        "#get probabilities for P(positive), P(negative) based on trimmed word counts\n",
        "allCount = posCount+negCount\n",
        "print(\"---------------Probabilities---------------\")\n",
        "print(\"P(positive) = %d/%d = %.5f\" %(posCount, allCount, posCount/allCount))\n",
        "print(\"P(negative) = %d/%d = %.5f\\n\" %(negCount, allCount, negCount/allCount))\n",
        "\n",
        "#get individual P(word|class) and store in dictionary\n",
        "def getProbs(Dict, PCount):\n",
        "  keys = list(Dict.keys())\n",
        "  vals = list(Dict.values())\n",
        "  newDict = dict()\n",
        "  for i in range(len(vals)):\n",
        "    k = keys[i]\n",
        "    p = vals[i]/PCount\n",
        "    newDict[k] = p\n",
        "  return newDict\n",
        "\n",
        "posProb = getProbs(posVocab, posCount)\n",
        "negProb = getProbs(negVocab, negCount)\n",
        "allProb = getProbs(allVocab, allCount)\n",
        "\n",
        "print(\"---------------P(word) Sample---------------\")\n",
        "print({k: allProb[k] for k in list(allProb)[:5]}, \"\\n\")\n",
        "\n",
        "print(\"---------------P(word|positive) Sample---------------\")\n",
        "print({k: posProb[k] for k in list(posProb)[:5]}, \"\\n\")\n",
        "\n",
        "print(\"---------------P(word|negative) Sample---------------\")\n",
        "print({k: negProb[k] for k in list(negProb)[:5]})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------Probabilities---------------\n",
            "P(positive) = 1883/31328 = 0.06011\n",
            "P(negative) = 29445/31328 = 0.93989\n",
            "\n",
            "---------------P(word) Sample---------------\n",
            "{'beijing': 0.0006064862104187947, 'controversial': 0.00015960163432073544, 'new': 0.0031601123595505617, 'security': 0.005905260469867212, 'law': 0.00450076608784474} \n",
            "\n",
            "---------------P(word|positive) Sample---------------\n",
            "{'beijing': 0.007434944237918215, 'controversial': 0.0026553372278279343, 'new': 0.0053106744556558685, 'security': 0.004248539564524694, 'law': 0.0053106744556558685} \n",
            "\n",
            "---------------P(word|negative) Sample---------------\n",
            "{'welcome': 0.0003056546102903719, 'disneyland': 0.00020376974019358125, 'hong': 0.06255731023942944, 'kong': 0.06262523348616064, 'enjoy': 0.00033961623365596876}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RATcYep5RsAj"
      },
      "source": [
        "# Testing Model with Dev Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4pgHOKOxT7s"
      },
      "source": [
        "The dev set of 160 items was used for the following:\n",
        "*   General accuracy testing of the fitted training model\n",
        "*   Five-fold cross validation to test the general algorithm.\n",
        "*   Accuracy testing with Laplace smoothing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Pl28efR_xa"
      },
      "source": [
        "**General Accuracy Testing**:\n",
        "\n",
        "Line by line, the process was:\n",
        "1.   Split into individual words\n",
        "2.   Determine and save the actual sentiment class\n",
        "3.   Remove words ≤ 2 characters in order to be consistent with the training data and prevent unnecessary zeroes. \n",
        "4.   Calculate a P(positive), P(negative) using the vocabulary and probability dictionaries created earlier.\n",
        "5.   Compare P(positive) and P(negative), taking the higher value as the predicted class.\n",
        "6.   Compared the predicted class with the actual class, and add on accordingly to a correct/wrong count.\n",
        "\n",
        "The final prediction accuracy was then calculated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sqg02UDDRxAT",
        "outputId": "780839cb-4c2f-4d99-ec63-289d614bd371"
      },
      "source": [
        "# print(\"---------------Sample Dev Set---------------\")\n",
        "# for i in range(7):\n",
        "#   print(dev[i], end='')\n",
        "\n",
        "from random import randrange\n",
        "\n",
        "def actualClass(line):\n",
        "  if line[-1] == '1':\n",
        "    return 1\n",
        "  elif line[-1] == '0':\n",
        "    return 0\n",
        "\n",
        "def splitTrim(line):\n",
        "  words = []\n",
        "  temp = re.findall(r'\\w+', line[0])\n",
        "  #print(temp)\n",
        "  for i in temp:\n",
        "    if len(i)>2:\n",
        "      words.append(i)\n",
        "  #print(words)\n",
        "  return words\n",
        "\n",
        "def naiveBayes(line):\n",
        "  words = splitTrim(line)\n",
        "  p_pos = 1\n",
        "  p_neg = 1\n",
        "  for word in words:\n",
        "    if word in posVocab:\n",
        "      p_pos = posProb[word]*p_pos\n",
        "    else:\n",
        "      p_pos = p_pos*0\n",
        "    if word in negVocab:\n",
        "      p_neg = negProb[word]*p_neg\n",
        "    else:\n",
        "      p_neg = p_neg*0\n",
        "  if p_pos > p_neg:\n",
        "    pred = 1\n",
        "  elif p_neg > p_pos:\n",
        "    pred = 0\n",
        "  else:\n",
        "    pred = 2\n",
        "  return pred\n",
        "\n",
        "def predict(data):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for line in data:\n",
        "    #print(line)\n",
        "    actual = actualClass(line)\n",
        "    pred = naiveBayes(line)\n",
        "    if pred == actual:\n",
        "      correct+=1\n",
        "      total+=1\n",
        "    else:\n",
        "      total+=1\n",
        "  acc = correct/total\n",
        "  return acc, correct, total\n",
        "\n",
        "##train on train set just for comparison\n",
        "accu, correct, total = predict(train)\n",
        "X.append(\"Train\")\n",
        "y.append(accu*100)\n",
        "\n",
        "accu, correct, total = predict(dev)\n",
        "print(\"Accuracy: %d/%d = %.2f\" %(correct, total, accu*100),\"%\")\n",
        "X.append(\"Dev\")\n",
        "y.append(accu*100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0/545 = 0.00 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JNu39gE0p_6"
      },
      "source": [
        "**Five-Fold Cross Validation**\n",
        "\n",
        "This tests the algorithm using cross validation. The process is as follows:\n",
        "1.   Split the data into 5 groups\n",
        "2.   For each group, use it as a test set with the remaining groups as training sets\n",
        "3.   Fit a model on the training set and evaluate on the test set\n",
        "4.   Repeat the above until all samples have had a turn as a test set\n",
        "5.   Find the mean accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hGF7Dg50ooD",
        "outputId": "c8413378-b009-41c6-c437-8e0456e63618"
      },
      "source": [
        "from random import randrange\n",
        "\n",
        "random.seed(1)\n",
        "\n",
        "def x_naiveBayes(line):\n",
        "  words = splitTrim(line)\n",
        "  p_pos = 1\n",
        "  p_neg = 1\n",
        "  for word in words:\n",
        "    if word in posVocab:\n",
        "      p_pos = posProb[word]*p_pos\n",
        "    else:\n",
        "      p_pos = p_pos*0\n",
        "    if word in negVocab:\n",
        "      p_neg = negProb[word]*p_neg\n",
        "    else:\n",
        "      p_neg = p_neg*0\n",
        "  if p_pos > p_neg:\n",
        "    pred = 1\n",
        "  elif p_neg > p_pos:\n",
        "    pred = 0\n",
        "  else:\n",
        "    pred = 2\n",
        "  return pred\n",
        "\n",
        "def x_predict(data):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for line in data:\n",
        "    #print(line)\n",
        "    actual = actualClass(line)\n",
        "    pred = x_naiveBayes(line)\n",
        "    if pred == actual:\n",
        "      correct+=1\n",
        "      total+=1\n",
        "    else:\n",
        "      total+=1\n",
        "  acc = correct/total\n",
        "  return acc, correct, total\n",
        "\n",
        "#split dataset into 5 folds\n",
        "def crossValsplit(dataset):\n",
        "  folds = 5\n",
        "  data_split = list()\n",
        "  data_copy = list(dataset)\n",
        "  fold_size = int(len(dataset)/folds)\n",
        "  for i in range(folds):\n",
        "    fold = list()\n",
        "    while len(fold) < fold_size:\n",
        "      index = randrange(len(data_copy))\n",
        "      fold.append(data_copy.pop(index))\n",
        "    data_split.append(fold)\n",
        "  return data_split\n",
        "\n",
        "def evaluate(data):\n",
        "  folds = crossValsplit(data)\n",
        "  scores = list()\n",
        "  for fold in folds:\n",
        "    train = list(folds)\n",
        "    train.remove(fold)\n",
        "    train = sum(train, [])\n",
        "    test = list()\n",
        "    for row in fold:\n",
        "      test.append(row)\n",
        "    #create new dictionaries based on fold\n",
        "    x_posStr, x_negStr = splitPosNeg(train)\n",
        "    x_posVocab, x_posCount = makeVocab(x_posStr)\n",
        "    x_negVocab, x_negCount = makeVocab(x_negStr)\n",
        "    x_allVocab = {**x_posVocab, **x_negVocab}\n",
        "    x_allCount = x_posCount+x_negCount\n",
        "    x_posProb = getProbs(x_posVocab, x_posCount)\n",
        "    x_negProb = getProbs(x_negVocab, x_negCount)\n",
        "    x_allProb = getProbs(x_allVocab, x_allCount)\n",
        "    acc, correct, total = x_predict(test)\n",
        "    acc = acc*100\n",
        "    scores.append(acc)\n",
        "  print(\"Scores: %s\" %scores)\n",
        "  print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n",
        "  X.append(\"Cross-Val\")\n",
        "  y.append(sum(scores)/float(len(scores)))\n",
        "    \n",
        "evaluate(dev)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Scores: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Mean Accuracy: 0.000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PzAk08VEsdj"
      },
      "source": [
        "Based on the mean accuracy from cross validation, the current algorithm is not very good and needs refinement before it can be tested against the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVOgzdecE6ZB"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXJVh-G8E8xD"
      },
      "source": [
        "**Laplace Smoothing**\n",
        "\n",
        "One major issue that Naive Bayes faces in text classification is missing data. When a word does not appear in a class, making P(word|class) = 0, that probability gets multiplied out to all the other probabilities in determining P(class), making P(class) = 0.\n",
        "\n",
        "Laplace smoothing is a method of combatting this issue. The process is:\n",
        "1.   Add 1 to every count so P(word|class) will always > 0.\n",
        "2.   Balance this by adding the number of possible words to the divisor so the result will never < 1.\n",
        "\n",
        "Smoothing is applied to the training set of 640 below, adding 1 to every unique word count and total words to #positive and #negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouPZeGCKIwnT",
        "outputId": "2c7bbb73-1b35-40fc-faac-5c47dcadac83"
      },
      "source": [
        "def trim(strings, tempDict):\n",
        "  keys = list(tempDict.keys())\n",
        "  vals = list(tempDict.values())\n",
        "  newDict = dict()\n",
        "  count = 0\n",
        "  for i in range(len(vals)):\n",
        "    k = keys[i]\n",
        "    if vals[i] >= 3 and len(k) > 2:\n",
        "      #smoothing: unique count + 1\n",
        "      newDict[k] = vals[i]+1\n",
        "      count+=vals[i]\n",
        "  return newDict, count\n",
        "\n",
        "def makeVocab(strings):\n",
        "  tempDict = dict()\n",
        "  splitWord(strings, tempDict)\n",
        "  newDict, count = trim(strings, tempDict)\n",
        "  return newDict, count\n",
        "\n",
        "posVocab, posCount = makeVocab(posStr)\n",
        "negVocab, negCount = makeVocab(negStr)\n",
        "\n",
        "allVocab = {**posVocab, **negVocab}\n",
        "\n",
        "#laplace smoothing\n",
        "posCount+=len(allVocab)\n",
        "negCount+=len(allVocab)\n",
        "allCount = posCount+negCount\n",
        "#for test words of P=0\n",
        "noPos = 1/posCount\n",
        "noNeg = 1/negCount\n",
        "\n",
        "def getProbs(Dict, PCount):\n",
        "  keys = list(Dict.keys())\n",
        "  vals = list(Dict.values())\n",
        "  newDict = dict()\n",
        "  for i in range(len(vals)):\n",
        "    k = keys[i]\n",
        "    p = vals[i]/PCount\n",
        "    newDict[k] = p\n",
        "  return newDict\n",
        "\n",
        "posProb = getProbs(posVocab, posCount)\n",
        "negProb = getProbs(negVocab, negCount)\n",
        "allProb = getProbs(allVocab, allCount)\n",
        "\n",
        "from random import randrange\n",
        "\n",
        "def actualClass(line):\n",
        "  if line[-1] == '1':\n",
        "    return 1\n",
        "  elif line[-1] == '0':\n",
        "    return 0\n",
        "\n",
        "def splitTrim(line):\n",
        "  words = []\n",
        "  temp = re.findall(r'\\w+', line[0])\n",
        "  #print(temp)\n",
        "  for i in temp:\n",
        "    if len(i)>2:\n",
        "      words.append(i)\n",
        "  #print(words)\n",
        "  return words\n",
        "\n",
        "def naiveBayes(line):\n",
        "  words = splitTrim(line)\n",
        "  p_pos = 1\n",
        "  p_neg = 1\n",
        "  for word in words:\n",
        "    if word in posVocab:\n",
        "      p_pos = posProb[word]*p_pos\n",
        "    else:\n",
        "      p_pos = p_pos*noPos\n",
        "    if word in negVocab:\n",
        "      p_neg = negProb[word]*p_neg\n",
        "    else:\n",
        "      p_neg = p_neg*noNeg\n",
        "  if p_pos > p_neg:\n",
        "    pred = 1\n",
        "  elif p_neg > p_pos:\n",
        "    pred = 0\n",
        "  return pred\n",
        "\n",
        "def predict(data):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for line in data:\n",
        "    #print(line)\n",
        "    actual = actualClass(line)\n",
        "    pred = naiveBayes(line)\n",
        "    if pred == actual:\n",
        "      correct+=1\n",
        "      total+=1\n",
        "    else:\n",
        "      total+=1\n",
        "  acc = correct/total\n",
        "  return acc, correct, total\n",
        "\n",
        "acc, correct, total = predict(dev)\n",
        "acc = acc*100\n",
        "print(\"Accuracy (Dev set with Smoothing): %d/%d = %.2f\" %(correct, total, acc),\"%\")\n",
        "X.append(\"Dev+\\nSmoothing\")\n",
        "y.append(acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy (Dev set with Smoothing): 106/545 = 19.45 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj_e9j7BNQWi"
      },
      "source": [
        "Applying smoothing dramatically improved the predictions' accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPSZb_q_OBO7"
      },
      "source": [
        "**Top Words**\n",
        "\n",
        "Next, I will derive the top 10 words for predicting class, ie. the top 10 each of P(Positive|word) and P(Negative|word). Given that:\n",
        "\n",
        "$P(class|word)=\\frac{P(word|class)P(class)}{P(word)}$\n",
        "\n",
        "and I already have the values for P(word), P(class), and P(word|class), this will be a simple matter of finding the top 10 values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0lCiZcKmQkQq",
        "outputId": "b577a74e-fc33-409e-a885-fe6d1de09f26"
      },
      "source": [
        "\"\"\"\n",
        "Find the top ten P(class|word) for each class\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# def topTen(classDict, classCount):\n",
        "#   words = []\n",
        "#   probs = []\n",
        "#   #keys = word, vals = P(word|class)\n",
        "#   keys = list(classDict.keys())\n",
        "#   vals = list(classDict.values())\n",
        "#   pClass = classCount/allCount\n",
        "#   for i in range(len(keys)):\n",
        "#     pWord = allProb[(keys[i])]\n",
        "#     p = (vals[i]*pClass)/pWord\n",
        "#     words.append(keys[i])\n",
        "#     probs.append(p)\n",
        "#   top = sorted(range(len(probs)), key=lambda i: probs[i])[-10:]\n",
        "#   topWords = []\n",
        "#   for i in top:\n",
        "#     topWords.append(words[i])\n",
        "#   return topWords\n",
        "\n",
        "# topPos = topTen(posProb, posCount)\n",
        "# topNeg = topTen(negProb, negCount)\n",
        "\n",
        "# print(\"Top Positive Words:\")\n",
        "# print(topPos)\n",
        "# print(\"\\nTop Negative Words:\")\n",
        "# print(topNeg)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nFind the top ten P(class|word) for each class\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovOGltlUTQ78"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPnCQrLAiRPq"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JarEUAwhiT3a"
      },
      "source": [
        "Finally, I will calculate the final accuracy. Based on the experiments above, using Laplace smoothing by itself provided the best accuracy, so that will be the approach I use here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROeMkHoihVIL",
        "outputId": "f61ed6e4-5959-4e4b-eec4-a5672becd6a3"
      },
      "source": [
        "from random import randrange\n",
        "\n",
        "def actualClass(line):\n",
        "  if line[-1] == '1':\n",
        "    return 1\n",
        "  elif line[-1] == '0':\n",
        "    return 0\n",
        "\n",
        "def splitTrim(line):\n",
        "  words = []\n",
        "  temp = re.findall(r'\\w+', line[0])\n",
        "  #print(temp)\n",
        "  for i in temp:\n",
        "    if len(i)>2:\n",
        "      words.append(i)\n",
        "  #print(words)\n",
        "  return words\n",
        "\n",
        "#using laplace where noPos and noNeg = 1/classCount\n",
        "def naiveBayes(line):\n",
        "  words = splitTrim(line)\n",
        "  p_pos = 1\n",
        "  p_neg = 1\n",
        "  for word in words:\n",
        "    if word in posVocab:\n",
        "      p_pos = posProb[word]*p_pos\n",
        "    else:\n",
        "      p_pos = p_pos*noPos\n",
        "    if word in negVocab:\n",
        "      p_neg = negProb[word]*p_neg\n",
        "    else:\n",
        "      p_neg = p_neg*noNeg\n",
        "  if p_pos > p_neg:\n",
        "    pred = 1\n",
        "  elif p_neg > p_pos:\n",
        "    pred = 0\n",
        "  return pred\n",
        "\n",
        "def predict(data):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for line in data:\n",
        "    #print(line)\n",
        "    actual = actualClass(line)\n",
        "    pred = naiveBayes(line)\n",
        "    if pred == actual:\n",
        "      correct+=1\n",
        "      total+=1\n",
        "    else:\n",
        "      total+=1\n",
        "  acc = correct/total\n",
        "  return acc, correct, total\n",
        "\n",
        "acc, correct, total = predict(test)\n",
        "acc = acc*100\n",
        "print(\"Accuracy: %d/%d = %.2f\" %(correct, total, acc),\"%\")\n",
        "X.append(\"Test\")\n",
        "y.append(acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 129/680 = 18.97 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNA2AqJulSF9"
      },
      "source": [
        "This has resulted in the highest accuracy of all the tests at 76.50%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arDa3y1ZTsjI"
      },
      "source": [
        "with open(\"hktwmacao.txt\", \"r\") as file:\n",
        "  text = file.read()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hxmr5T3UCPG",
        "outputId": "26b4fffa-b056-4afe-c273-e1218ce8f40c"
      },
      "source": [
        "import string\n",
        "import collections\n",
        "\n",
        "# get rid of all the XML markup\n",
        "text = re.sub('<.*>','',text)\n",
        "\n",
        "# get rid of the \"ENDOFARTICLE.\" text\n",
        "text = re.sub('ENDOFARTICLE.','',text)\n",
        "\n",
        "# get rid of punctuation (except periods!)\n",
        "punctuation = \"[\" + re.sub(\"\\\"\",\"\",string.punctuation) + \"]\"\n",
        "text = re.sub(punctuation, \"\", text)\n",
        "\n",
        "tokenized = text.split()\n",
        "\n",
        "# and get a list of all the bi-grams\n",
        "bigrams = ngrams(tokenized, 2)\n",
        "yongrams = ngrams(tokenized, 4)\n",
        "gograms = ngrams(tokenized, 5)\n",
        "\n",
        "top2 = collections.Counter(ngrams(tokenized, 2)).most_common(10)\n",
        "top4 = collections.Counter(ngrams(tokenized, 4)).most_common(10)\n",
        "top5 = collections.Counter(ngrams(tokenized, 5)).most_common(10)\n",
        "\n",
        "top5\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('Hong', 'Kong', 'Special', 'Administrative', 'Region'), 239),\n",
              " (('Kong', 'Special', 'Administrative', 'Region', 'HKSAR'), 191),\n",
              " (('the', 'Hong', 'Kong', 'Special', 'Administrative'), 163),\n",
              " (('of', 'the', 'Hong', 'Kong', 'Special'), 105),\n",
              " (('the', 'Standing', 'Committee', 'of', 'the'), 85),\n",
              " (('Special', 'Administrative', 'Region', 'HKSAR', 'government'), 70),\n",
              " (('of', 'the', 'National', 'Peoples', 'Congress'), 61),\n",
              " (('Hong', 'Kongs', 'Center', 'for', 'Health'), 60),\n",
              " (('Kongs', 'Center', 'for', 'Health', 'Protection'), 60),\n",
              " (('Standing', 'Committee', 'of', 'the', 'National'), 57)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYg5bqyjlYLS"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2XeAbWplaGj"
      },
      "source": [
        "In all the tests, only the ones that made use of Laplace smoothing had accuracies > 50%. This clearly shows one of the limitations of this particular algorithm: unknown and unaccounted-for data will have their probabilities of zero multiply out, rendering many classification attempts useless. However, by simply incorporating smoothing, this issue is largely resolved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kM7V_h9mT80"
      },
      "source": [
        "**Future Possibilities**\n",
        "\n",
        "In my tests, I used certain trimming methods on the datasets (frequency > 3, word length > 2 characters). Other possible tests could experiment with different values of the two. Alternatively, instead of setting a frequency/length minimum, I could just import a list of stopwords such as that available from Natural Language Toolkit's [corpus](https://pythonprogramming.net/stop-words-nltk-tutorial/) and remove any stopwords from the datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UvedlFEeXnk"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEBB5Z5GeZyj"
      },
      "source": [
        "*   Bruno Stecanella, \"A practical explanation of a Naive Bayes classifier\", https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/\n",
        "*   Jason Brownlee, \"Naive Bayes Classifier From Scratch in Python\n",
        "\", https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\n",
        "\n",
        "\n"
      ]
    }
  ]
}